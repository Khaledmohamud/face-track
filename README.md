# Face-Track

## What is it

Face-Track is a computer vision-powered project with the purpose of giving the average person access to a live production suite through the use of a virtual camera. Understanding that not all cameras are production quality with the ability to focus on a user and adjust zoom through the hardware, Face-Track is designed to take in raw video footage and in real time, splice and zoom into human faces, following and tracking them almost as though the camera is being operated by a different person.

## Motivations

1. **Messy room**  
   I made this project during a very busy time in my life, very often spending time in the library I left my room in a bit of disarray, and when I would get a second of peace from all my work, all the commentary I would receive would be directly related to the state of my domicile.  
   To counter this I was motivated to create a camera that would focus on the one thing that mattered when I was on camera ‚Äî me.

2. **Challenging myself**  
   Ask any dev and they will agree, sometimes you hit a creative block that just completely stunts any creativity, especially with regard to creating new projects. Personally, I never believed I suffered from this, but during this time I realised I was slowly becoming what I feared ‚Äî an engineer that built to meet deadlines, not to create, to inspire, to enjoy.  
   I feared I was losing my passion as it slowly turned to work, and so I built this project as a challenge.

3. **Accessibility**  
   Though this project initially started out as a solution for myself (the usual motivation for all my projects), it slowly but surely turned into a project my friends wanted to get their hands on. This was heavily due to the lack of access many of them had to hardware capable of doing what my project could.  
   So I incrementally built onto it, developing it to better support people around the world.

## Current status

The project has been my personal baby for 6 months and I have always been very protective of it, fearing distributing it. However, at the moment, after multiple iterations I feel confident in releasing it for public use.  
The project can track human faces and zoom in with the gestures "ü§ü" and "‚úåÔ∏è". Its latest iteration has background blur and live captioning, giving room to reach a more accessible audience who may be deaf or audiences who may suffer due to a language barrier.

## Future adaptations

Currently I have personally identified various adaptations that can be made to the project enhancing the experience. These are:

1. **Split screening**  
   Currently if there are 2 people on camera, the camera jumps back and forth between the users, making it hard to use. But with split screening it could be possible to have both users represented on one video output.

2. **Custom voice tracking**  
   The live captions only identify speech and do not identify speakers. Using both auditory and visual data, the project could in future track the speakers and assign them custom caption colors and titles (e.g. "speaker-1" and "speaker-2").
