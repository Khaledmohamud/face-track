<h1>Face-Track</h1>

<h2>What is it</h2>

Face-Track is a computer vision-powered project with the purpose of giving the avarage person access to a live production suite similar to through the use of a virtual camera. Understanding that not all cameras are production quality with the ability to focus on a user and adjust zoom through the hardware.
Face-Track is designed to take in raw video footage and in real time, splice and zoom into human faces, following and tracking them almost as though the camera is being operated by a different person.

<h2> Motivations</h2>
1. messy room - I made this project during a very busy time in my life, very often spending time in the library I left my room in a bit of disarray, and when I would get a second of peace from all my work, all the commentary I would recieve, would be directly related to the state of my domicile.
to counter this I was motivated to create a camera that would focus on the one thing that mattered when I was on camera, me.
2. Challenging myself - Ask any dev and they will agree, sometimes you hit a creative block that just completely stunts any creativity, especially with regard to creating new projects, personally I never believed I suufered from this, but during this time I realised I was slowly becoming what I feared, an engineer that built to meet deadlines, not to create, to inspire, to enjoy.
I feared I was losing my passion as it slowly turned to work, and so I built this project as a challenge
3. accessibility - Though this project initially started out as a solution for myself(the usual motivations for all my projects) it slowly but surely turned into a project my friends wanted to get their hands on, this was heavily due to the lack of access many of them had to hardware capable of doing what my project could.
So Incrementally built onto it, developping it to better support people around the world.

<h2> Current status</h2>

The project has been my personal baby for 6 months and I have always been very protective of it fearing distributing it, however at the moment, after multiple iterations I feel confident in releasing it for public use. the project can track human faces zoom in with the gestures, "ü§ü" and "‚úåÔ∏è"", it's latest iteration has background blur and live captioning, giving room to reach a more accessible audience who may be deaf or audiences who may suffer due to a language barrier.

<h2> Future adaptations</hw>
Currently I have personally identified various adaptations that can be made to the project enhancing the experience. these are:
1. Split screening - currently if there is 2 people on camera, the camera jumps back and fourth between the users, making it hard to use. but with split screening it could be possible to have both useres represented on on video output.
2. Custom voice tracking - the live captions only identify speech and do not identify speakers, using both auditory and visual data the project could in future track the speakers and assign them custom caption colors and titles i.e ("speaker-1" and "speaker-2")
